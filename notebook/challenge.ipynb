{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7acf6d",
   "metadata": {},
   "source": [
    "# Challenge: Edit MNIST challenge images to be correctly predicted\n",
    "\n",
    "Goal: Edit the images placed in `data/MNIST/challenge` so that the provided model predicts the correct label while keeping more than 60% of the original pixels unchanged.\n",
    "\n",
    "Description: You are given a pre-trained `SmallCNN` model and a small set of challenge images. Your task is to minimally modify these images so the model classifies them correctly. This exercise encourages you to: \n",
    "- Explore the sample dataset in `data/MNIST/sample` to understand variation and typical inputs.\n",
    "- Use explainable AI (XAI) techniques (saliency maps, Grad-CAM, Integrated Gradients, occlusion, etc.) to discover what parts of the image the model relies on.\n",
    "- Propose minimal edits (pixel changes, small masks, subtle color shifts) that change model prediction while preserving at least 60% of the original pixels.\n",
    "\n",
    "Deliverables: For each edited image, save the modified image to `data/MNIST/challenge/edited/` alongside a short report (less than 2 pages) describing the XAI insights you used and the percentage of pixels preserved. For ease of use, you have the images already in that folder and you can directly work on them. You are allowed to use any external program you want to modify the image (i.e., paint, photoshop, figma, ...).\n",
    "\n",
    "The practice can be done by more than 1 person. Final grade would depend on the number of images correctly edited (n_images_correct) with their corresponding report and number of persons working together (n_persons) following the next formula:\n",
    "$$grade = 2,5 \\times n\\_images\\_correct - 2,5 * (n\\_persons - 1)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "281c72a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports and device\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f12e6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SmallCNN definition (must match the trained model architecture)\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 12, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(12*7*7, 12)\n",
    "        self.fc2 = nn.Linear(12, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))  # 14x14\n",
    "        x = self.pool(x)                      # 7x7\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(torch.nn.functional.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# convenience transform\n",
    "to_tensor = transforms.ToTensor()\n",
    "to_pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8698f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallCNN(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (fc1): Linear(in_features=588, out_features=12, bias=True)\n",
       "  (fc2): Linear(in_features=12, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dir = Path('../models')\n",
    "model_name = models_dir / 'small_cnn.pth'\n",
    "model = torch.load(model_name, weights_only=False, map_location=device)\n",
    "model.to(device) \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58b91e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_label7.png: Pred: 3, Label: 7\n",
      "1_label3.png: Pred: 1, Label: 3\n",
      "4_label2.png: Pred: 6, Label: 2\n",
      "2_label3.png: Pred: 1, Label: 3\n",
      "0_label5.png: Pred: 1, Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the images in data/MNIST/challenge/ and plot them. Label of the image is the last character of the filename.\n",
    "challenge_dir = Path('../data/MNIST/challenge')\n",
    "image_files = list(challenge_dir.glob('*.png'))\n",
    "fig, axes = plt.subplots(1, len(image_files), figsize=(12,4))\n",
    "for ax, img_file in zip(axes, image_files):\n",
    "    img = to_tensor(Image.open(img_file)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "        pred = output.argmax(dim=1).item()\n",
    "    ax.imshow(to_pil(img.squeeze().cpu()))\n",
    "    ax.set_title(f'Pred: {pred}, label: {img_file.stem[-1]}')\n",
    "    ax.axis('off')\n",
    "    print(f'{img_file.name}: Pred: {pred}, Label: {img_file.stem[-1]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc11179",
   "metadata": {},
   "source": [
    "---\n",
    "### Any code you want to add, put it below this markdown cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "805bb1c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot register a hook on a tensor that doesn't require gradient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_ch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m: x \u001b[38;5;241m=\u001b[39m x[:\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     73\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 75\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     77\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(img_path\u001b[38;5;241m.\u001b[39mstem[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [4], line 12\u001b[0m, in \u001b[0;36mSmallCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))  \u001b[38;5;66;03m# 14x14\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)                      \u001b[38;5;66;03m# 7x7\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1616\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1614\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1616\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1619\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torchcam/methods/gradient.py:49\u001b[0m, in \u001b[0;36m_GradCAM._hook_g\u001b[0;34m(self, module, input, output, idx)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m\"\"\"Gradient hook\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hooks_enabled:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_handles\u001b[38;5;241m.\u001b[39mappend(\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/_tensor.py:561\u001b[0m, in \u001b[0;36mTensor.register_hook\u001b[0;34m(self, hook)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39mregister_hook, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, hook)\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot register a hook on a tensor that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt require gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot register a hook on a tensor that doesn't require gradient"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "# --- utils ---\n",
    "def first_conv_in_ch(model: nn.Module) -> int:\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d): return m.in_channels\n",
    "    raise RuntimeError(\"No Conv2d in model.\")\n",
    "\n",
    "def norm01(t: torch.Tensor) -> torch.Tensor:\n",
    "    t = t - t.min(); m = t.max()\n",
    "    return t/m if m > 0 else t\n",
    "\n",
    "def overlay(img_pil: Image.Image, heat_hw: np.ndarray, alpha=0.55):\n",
    "    img_rgb = img_pil.convert(\"RGB\")\n",
    "    H, W = img_rgb.size[1], img_rgb.size[0]\n",
    "    if heat_hw.shape != (H, W):\n",
    "        heat_hw_img = Image.fromarray((np.clip(heat_hw,0,1)*255).astype(np.uint8)).resize((W,H), Image.BILINEAR)\n",
    "        heat_hw = np.asarray(heat_hw_img)/255.0\n",
    "    cmap = plt.get_cmap(\"jet\")(heat_hw)[..., :3]\n",
    "    base = np.asarray(img_rgb, dtype=np.float32)/255.0\n",
    "    out = np.clip(alpha*cmap + (1-alpha)*base, 0, 1)\n",
    "    return (out*255).astype(np.uint8)\n",
    "\n",
    "@torch.no_grad()\n",
    "def rise_saliency(model, x, target_class, N=2000, s=7, p=0.5, batch_size=128):\n",
    "    \"\"\"\n",
    "    x: (1,C,H,W) en [0,1] en device\n",
    "    N: nº de máscaras, s: granularidad, p: prob de mantener celda\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    _, C, H, W = x.shape\n",
    "    h = int(np.ceil(H / s)); w = int(np.ceil(W / s))\n",
    "    sal = torch.zeros(1, 1, H, W, device=x.device)\n",
    "\n",
    "    for start in range(0, N, batch_size):\n",
    "        b = min(batch_size, N - start)\n",
    "        masks = torch.bernoulli(torch.full((b, 1, h, w), p, device=x.device))\n",
    "        masks = F.interpolate(masks, size=(H, W), mode='bilinear', align_corners=False)\n",
    "        x_rep = x.repeat(b, 1, 1, 1) * masks.repeat(1, C, 1, 1)\n",
    "        scores = model(x_rep)[:, target_class]        # (b,)\n",
    "        sal += torch.einsum('b,bchw->chw', scores, masks).unsqueeze(0)\n",
    "\n",
    "    sal = sal / (N * p)\n",
    "    sal = norm01(sal.squeeze()).cpu().numpy()         # (H,W) [0,1]\n",
    "    return sal\n",
    "\n",
    "# --- parámetros/paths ---\n",
    "challenge_dir = Path('../data/MNIST/challenge')\n",
    "image_files = sorted(challenge_dir.glob('*.png'))\n",
    "assert image_files, \"No hay PNGs en ../data/MNIST/challenge\"\n",
    "\n",
    "in_ch = first_conv_in_ch(model)\n",
    "TARGET   = 'pred'   # 'pred' o 'label'\n",
    "N_MASKS  = 2000\n",
    "CELL_S   = 7\n",
    "KEEP_P   = 0.5\n",
    "BATCH    = 128\n",
    "TOP_PCT  = 10\n",
    "\n",
    "# --- ejecutar una a una ---\n",
    "for img_path in image_files:\n",
    "    img_pil = Image.open(img_path).convert('L')\n",
    "    x = to_tensor(img_pil)                       # (1,H,W) [0,1]\n",
    "    if in_ch == 3 and x.shape[0] == 1: x = x.repeat(3,1,1)\n",
    "    if in_ch == 1 and x.shape[0] == 3: x = x[:1]\n",
    "    x = x.unsqueeze(0).to(device)\n",
    "\n",
    "    logits = model(x)\n",
    "    pred = int(torch.argmax(logits, dim=1).item())\n",
    "    label = int(img_path.stem[-1])\n",
    "    target = pred if TARGET == 'pred' else label\n",
    "\n",
    "    heat = rise_saliency(model, x, target, N=N_MASKS, s=CELL_S, p=KEEP_P, batch_size=BATCH)\n",
    "\n",
    "    thr = np.percentile(heat, 100 - TOP_PCT)\n",
    "    top_mask = (heat >= thr)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(9,3))\n",
    "    axs[0].imshow(img_pil, cmap='gray'); axs[0].set_title(f'Original\\nPred:{pred} | Label:{label}'); axs[0].axis('off')\n",
    "    axs[1].imshow(overlay(img_pil, heat)); axs[1].set_title(f'RISE ({TARGET})'); axs[1].axis('off')\n",
    "\n",
    "    rgb = img_pil.convert(\"RGB\")\n",
    "    base = np.asarray(rgb, dtype=np.float32)/255.0\n",
    "    red = base.copy(); red[top_mask] = [1.0, 0.0, 0.0]\n",
    "    axs[2].imshow(np.clip(0.6*red + 0.4*base, 0, 1)); axs[2].set_title(f'Top {TOP_PCT}%'); axs[2].axis('off')\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "    print(f\"{img_path.name}: Pred={pred}, Label={label}, Explicado={TARGET} (clase {target})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d670384a",
   "metadata": {},
   "source": [
    "---\n",
    "## Check if you have passed the challenge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039eae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check percentage of editing an image\n",
    "def calculate_edit_percentage(original_img, edited_img):\n",
    "    original_pixels = original_img.load()\n",
    "    edited_pixels = edited_img.load()\n",
    "    width, height = original_img.size\n",
    "    total_pixels = width * height\n",
    "    changed_pixels = 0\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            if original_pixels[x, y] != edited_pixels[x, y]:\n",
    "                changed_pixels += 1\n",
    "\n",
    "    return (changed_pixels / total_pixels) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da710de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create edited directory\n",
    "edited_dir = challenge_dir / 'edited'\n",
    "\n",
    "# Load edited images, check that they are predicted correctly and calculate edit percentages\n",
    "for original_img_file, edited_img_file in zip(challenge_dir.glob('*.png'), edited_dir.glob('*.png')):\n",
    "    original_img = Image.open(original_img_file)\n",
    "    edited_img = Image.open(edited_img_file)\n",
    "    # Convert the edited image to RGB if it's not\n",
    "    if edited_img.mode != 'RGB':\n",
    "        edited_img = edited_img.convert('RGB')\n",
    "\n",
    "    # Check prediction\n",
    "    img_tensor = to_tensor(edited_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        pred = output.argmax(dim=1).item()\n",
    "    \n",
    "    print(f'Edited {edited_img_file.name}: Pred: {pred}, Label: {original_img_file.stem[-1]}, correct: {pred == int(original_img_file.stem[-1])}')\n",
    "\n",
    "    # Calculate edit percentage\n",
    "    edit_percentage = calculate_edit_percentage(original_img, edited_img)\n",
    "    print(f'Edit Percentage: {edit_percentage:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
